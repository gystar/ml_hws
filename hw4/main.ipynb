{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentiment\n",
    "import utils\n",
    "from os import path\n",
    "importlib.reload(sentiment)\n",
    "importlib.reload(utils)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuda is available\nCuda is available\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mist/ml_hws/hw4/utils.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = path.dirname(path.abspath(\"__file__\"))\n",
    "TRAIN_LABEL_PATH = path.join(current_dir,\"data/training_label.txt\")\n",
    "TRAIN_NO_LABEL_PATH = path.join(current_dir,\"data/training_nolabel.txt\")\n",
    "TESTING_PATH = path.join(current_dir,\"data/testing_data.txt\")\n",
    "\n",
    "f = open(TRAIN_LABEL_PATH, encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "labels_train = np.array([int(line.split(\" \")[0]) for line in lines])\n",
    "sentenses_labeled = np.array(\n",
    "    [line.strip(\"\\n\").split(\" \")[2:] for line in lines])\n",
    "\n",
    "f = open(TRAIN_NO_LABEL_PATH, encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "sentenses_nolabeled = np.array(\n",
    "    [line.strip(\"\\n\").split(\" \") for line in lines])\n",
    "\n",
    "f = open(TESTING_PATH, encoding='utf-8')\n",
    "# 去掉第一行->去掉每一行的标号->去掉\\n\n",
    "lines = f.readlines()[1:]\n",
    "sentenses_test = np.array([line.split(\",\", 1)[1].strip(\"\\n\").split(\" \")\n",
    "                           for line in lines])"
   ]
  },
  {
   "source": [
    "# word embedding，利用所有的语料库，使用word2vec建立词向量，每个key的词向量存储在model.wv[key]中\n",
    "DIM_WORD = 1024  # 词向量的维度，若改变，需要手动重新生成词向量库\n",
    "SAVED_PATH = path.join(current_dir,\"wordvec.pkl\")\n",
    "try :  # 若已经保存过，则不需要再训练词向量，因为生成较慢\n",
    "    wordvecs = Word2Vec.load(SAVED_PATH)\n",
    "    print(\"load wordvecs from file.\")\n",
    "except:  # 没有保存过，则生成词向量并存储\n",
    "    print(\"generating word vectors...\")\n",
    "    all_sentenses = np.concatenate((sentenses_labeled, sentenses_nolabeled, sentenses_test), axis=0)\n",
    "    wordvecs = Word2Vec(all_sentenses, size=DIM_WORD, window=5, min_count=1,\n",
    "                        workers=multiprocessing.cpu_count())\n",
    "    wordvecs.save(SAVED_PATH)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generating word vectors...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " loss: 0.454256931709295\n",
      "120000  sentenses input, avarage loss: 0.453452135482803\n",
      "130000  sentenses input, avarage loss: 0.454761807783865\n",
      "140000  sentenses input, avarage loss: 0.4553636395478887\n",
      "150000  sentenses input, avarage loss: 0.45331114533295236\n",
      "160000  sentenses input, avarage loss: 0.4509635568433441\n",
      "170000  sentenses input, avarage loss: 0.4507309050923761\n",
      "180000  sentenses input, avarage loss: 0.4518539153598249\n",
      "190000  sentenses input, avarage loss: 0.4526959391310811\n",
      "200000  sentenses input, avarage loss: 0.4527043503616005\n",
      "[epochs 10 / 50 ], avarage loss: 0.4527043503616005\n",
      "10000  sentenses input, avarage loss: 0.4484521523863077\n",
      "20000  sentenses input, avarage loss: 0.4558925307355821\n",
      "30000  sentenses input, avarage loss: 0.46809072232494753\n",
      "40000  sentenses input, avarage loss: 0.46627887195907536\n",
      "50000  sentenses input, avarage loss: 0.4599189158603549\n",
      "60000  sentenses input, avarage loss: 0.45112597186739245\n",
      "70000  sentenses input, avarage loss: 0.45181549543248756\n",
      "80000  sentenses input, avarage loss: 0.4476124980626628\n",
      "90000  sentenses input, avarage loss: 0.44903795710454386\n",
      "100000  sentenses input, avarage loss: 0.44537223868072034\n",
      "110000  sentenses input, avarage loss: 0.4457290614193136\n",
      "120000  sentenses input, avarage loss: 0.4459073735338946\n",
      "130000  sentenses input, avarage loss: 0.44998070830335984\n",
      "140000  sentenses input, avarage loss: 0.44997326259634324\n",
      "150000  sentenses input, avarage loss: 0.45065467272202175\n",
      "160000  sentenses input, avarage loss: 0.4472520546452142\n",
      "170000  sentenses input, avarage loss: 0.44803165148067126\n",
      "180000  sentenses input, avarage loss: 0.44637055343016985\n",
      "190000  sentenses input, avarage loss: 0.4448663481344518\n",
      "200000  sentenses input, avarage loss: 0.44547582055442037\n",
      "[epochs 11 / 50 ], avarage loss: 0.44547582055442037\n",
      "10000  sentenses input, avarage loss: 0.4418763196468353\n",
      "20000  sentenses input, avarage loss: 0.4359178202971816\n",
      "30000  sentenses input, avarage loss: 0.4272060255395869\n",
      "40000  sentenses input, avarage loss: 0.4354727541422471\n",
      "50000  sentenses input, avarage loss: 0.42914651734754444\n",
      "60000  sentenses input, avarage loss: 0.43774824633883935\n",
      "70000  sentenses input, avarage loss: 0.44065520659887364\n",
      "80000  sentenses input, avarage loss: 0.4396117024472915\n",
      "90000  sentenses input, avarage loss: 0.43635624973103404\n",
      "100000  sentenses input, avarage loss: 0.4361719152685255\n",
      "110000  sentenses input, avarage loss: 0.43978998588059437\n",
      "120000  sentenses input, avarage loss: 0.4374174555096154\n",
      "130000  sentenses input, avarage loss: 0.43796579954286036\n",
      "140000  sentenses input, avarage loss: 0.4379054357324328\n",
      "150000  sentenses input, avarage loss: 0.43774210998415947\n",
      "160000  sentenses input, avarage loss: 0.4414332760870457\n",
      "170000  sentenses input, avarage loss: 0.4415723573394558\n",
      "180000  sentenses input, avarage loss: 0.4397556540908085\n",
      "190000  sentenses input, avarage loss: 0.43933032267972044\n",
      "200000  sentenses input, avarage loss: 0.4393127549663186\n",
      "[epochs 12 / 50 ], avarage loss: 0.4393127549663186\n",
      "10000  sentenses input, avarage loss: 0.42140346236526965\n",
      "20000  sentenses input, avarage loss: 0.4118663230538368\n",
      "30000  sentenses input, avarage loss: 0.4392418763041496\n",
      "40000  sentenses input, avarage loss: 0.43344589250162247\n",
      "50000  sentenses input, avarage loss: 0.43633178374171255\n",
      "60000  sentenses input, avarage loss: 0.44601644947504004\n",
      "70000  sentenses input, avarage loss: 0.44012866717896293\n",
      "80000  sentenses input, avarage loss: 0.4430194532359019\n",
      "90000  sentenses input, avarage loss: 0.44485651339507765\n",
      "100000  sentenses input, avarage loss: 0.4436357337422669\n",
      "110000  sentenses input, avarage loss: 0.4443336303498257\n",
      "120000  sentenses input, avarage loss: 0.44114511482107144\n",
      "130000  sentenses input, avarage loss: 0.44463424500077964\n",
      "140000  sentenses input, avarage loss: 0.44591549421793647\n",
      "150000  sentenses input, avarage loss: 0.44637360925972464\n",
      "160000  sentenses input, avarage loss: 0.44531331303995103\n",
      "170000  sentenses input, avarage loss: 0.44674229591646614\n",
      "180000  sentenses input, avarage loss: 0.44579303237713047\n",
      "190000  sentenses input, avarage loss: 0.44507140161567615\n",
      "200000  sentenses input, avarage loss: 0.4441428688876331\n",
      "[epochs 13 / 50 ], avarage loss: 0.4441428688876331\n",
      "10000  sentenses input, avarage loss: 0.46822479825466873\n",
      "20000  sentenses input, avarage loss: 0.48408285340294244\n",
      "30000  sentenses input, avarage loss: 0.46894859640548625\n",
      "40000  sentenses input, avarage loss: 0.4772917328123003\n",
      "50000  sentenses input, avarage loss: 0.47492211338132617\n",
      "60000  sentenses input, avarage loss: 0.47216566286360223\n",
      "70000  sentenses input, avarage loss: 0.46985230916312765\n",
      "80000  sentenses input, avarage loss: 0.4632388134207577\n",
      "90000  sentenses input, avarage loss: 0.4630660752620962\n",
      "100000  sentenses input, avarage loss: 0.4596638699322939\n",
      "110000  sentenses input, avarage loss: 0.45462688121944667\n",
      "120000  sentenses input, avarage loss: 0.45061892238135137\n",
      "130000  sentenses input, avarage loss: 0.44834840826117073\n",
      "140000  sentenses input, avarage loss: 0.4465151904709637\n",
      "150000  sentenses input, avarage loss: 0.4457813462764025\n",
      "160000  sentenses input, avarage loss: 0.4455211556330323\n",
      "170000  sentenses input, avarage loss: 0.4458617871035548\n",
      "180000  sentenses input, avarage loss: 0.4441157738864422\n",
      "190000  sentenses input, avarage loss: 0.4424207016080618\n",
      "200000  sentenses input, avarage loss: 0.4446244821920991\n",
      "[epochs 14 / 50 ], avarage loss: 0.4446244821920991\n",
      "10000  sentenses input, avarage loss: 0.4553559685498476\n",
      "20000  sentenses input, avarage loss: 0.4608450493961573\n",
      "30000  sentenses input, avarage loss: 0.4566397291918596\n",
      "40000  sentenses input, avarage loss: 0.4516150225326419\n",
      "50000  sentenses input, avarage loss: 0.44528193420916795\n",
      "60000  sentenses input, avarage loss: 0.4460511519946158\n",
      "70000  sentenses input, avarage loss: 0.4437853466240423\n",
      "80000  sentenses input, avarage loss: 0.4459300270164385\n",
      "90000  sentenses input, avarage loss: 0.44294550788485343\n",
      "100000  sentenses input, avarage loss: 0.43760154488310216\n",
      "110000  sentenses input, avarage loss: 0.43740736032074146\n",
      "120000  sentenses input, avarage loss: 0.4386355041526258\n",
      "130000  sentenses input, avarage loss: 0.43909358866512777\n",
      "140000  sentenses input, avarage loss: 0.44269447407552176\n",
      "150000  sentenses input, avarage loss: 0.4423211916834116\n",
      "160000  sentenses input, avarage loss: 0.4448138548992574\n",
      "170000  sentenses input, avarage loss: 0.4455725210357238\n",
      "180000  sentenses input, avarage loss: 0.4453812143061724\n",
      "190000  sentenses input, avarage loss: 0.4457873427809069\n",
      "200000  sentenses input, avarage loss: 0.4482864461075515\n",
      "[epochs 15 / 50 ], avarage loss: 0.4482864461075515\n",
      "10000  sentenses input, avarage loss: 0.4559995523095131\n",
      "20000  sentenses input, avarage loss: 0.4376794363185763\n",
      "30000  sentenses input, avarage loss: 0.4439454328765472\n",
      "40000  sentenses input, avarage loss: 0.44804393883794547\n",
      "50000  sentenses input, avarage loss: 0.4466893962174654\n",
      "60000  sentenses input, avarage loss: 0.45482084788382054\n",
      "70000  sentenses input, avarage loss: 0.45518471850880554\n",
      "80000  sentenses input, avarage loss: 0.4494530621683225\n",
      "90000  sentenses input, avarage loss: 0.44436427428904507\n",
      "100000  sentenses input, avarage loss: 0.4449887837730348\n",
      "110000  sentenses input, avarage loss: 0.4449945795976303\n",
      "120000  sentenses input, avarage loss: 0.44291842729163666\n",
      "130000  sentenses input, avarage loss: 0.4411991904733273\n",
      "140000  sentenses input, avarage loss: 0.4381338242867163\n",
      "150000  sentenses input, avarage loss: 0.4374361049036185\n",
      "160000  sentenses input, avarage loss: 0.43604607354151087\n",
      "170000  sentenses input, avarage loss: 0.43756560664185706\n",
      "180000  sentenses input, avarage loss: 0.4386421803757548\n",
      "190000  sentenses input, avarage loss: 0.436426074879342\n",
      "200000  sentenses input, avarage loss: 0.4383525591418147\n",
      "[epochs 16 / 50 ], avarage loss: 0.4383525591418147\n",
      "10000  sentenses input, avarage loss: 0.4427677547186613\n",
      "20000  sentenses input, avarage loss: 0.44276980105787517\n",
      "30000  sentenses input, avarage loss: 0.43637721178432304\n",
      "40000  sentenses input, avarage loss: 0.44208222514018414\n",
      "50000  sentenses input, avarage loss: 0.4431141436100006\n",
      "60000  sentenses input, avarage loss: 0.4451992061237494\n",
      "70000  sentenses input, avarage loss: 0.43645907827786035\n",
      "80000  sentenses input, avarage loss: 0.43824477890506386\n",
      "90000  sentenses input, avarage loss: 0.44354204944852327\n",
      "100000  sentenses input, avarage loss: 0.44055798021331427\n",
      "110000  sentenses input, avarage loss: 0.44013948808000847\n",
      "120000  sentenses input, avarage loss: 0.44106270009962223\n",
      "130000  sentenses input, avarage loss: 0.4406867998494552\n",
      "140000  sentenses input, avarage loss: 0.4388557635300926\n",
      "150000  sentenses input, avarage loss: 0.43860377837965886\n",
      "160000  sentenses input, avarage loss: 0.43650813445448877\n",
      "170000  sentenses input, avarage loss: 0.4361105438979233\n",
      "180000  sentenses input, avarage loss: 0.43773587611400416\n",
      "190000  sentenses input, avarage loss: 0.43748601474652166\n",
      "200000  sentenses input, avarage loss: 0.43720314357057216\n",
      "[epochs 17 / 50 ], avarage loss: 0.43720314357057216\n",
      "10000  sentenses input, avarage loss: 0.4463578384369612\n",
      "20000  sentenses input, avarage loss: 0.4476140127889812\n",
      "30000  sentenses input, avarage loss: 0.44088084865361454\n",
      "40000  sentenses input, avarage loss: 0.442449276028201\n",
      "50000  sentenses input, avarage loss: 0.44140113063901665\n",
      "60000  sentenses input, avarage loss: 0.4422646230645478\n",
      "70000  sentenses input, avarage loss: 0.44621478912021434\n",
      "80000  sentenses input, avarage loss: 0.4469071402773261\n",
      "90000  sentenses input, avarage loss: 0.44551154772440593\n",
      "100000  sentenses input, avarage loss: 0.4498652850314975\n",
      "110000  sentenses input, avarage loss: 0.44952518749643455\n",
      "120000  sentenses input, avarage loss: 0.4456514116687079\n",
      "130000  sentenses input, avarage loss: 0.44403423162893607\n",
      "140000  sentenses input, avarage loss: 0.4382617558698569\n",
      "150000  sentenses input, avarage loss: 0.43688333507378896\n",
      "160000  sentenses input, avarage loss: 0.4379942941607442\n",
      "170000  sentenses input, avarage loss: 0.43874930731404355\n",
      "180000  sentenses input, avarage loss: 0.44150944363222355\n",
      "190000  sentenses input, avarage loss: 0.4428559457147984\n",
      "200000  sentenses input, avarage loss: 0.4428780254414305\n",
      "[epochs 18 / 50 ], avarage loss: 0.4428780254414305\n",
      "10000  sentenses input, avarage loss: 0.4440819092094898\n",
      "20000  sentenses input, avarage loss: 0.4266091782040894\n",
      "30000  sentenses input, avarage loss: 0.4217477147653699\n",
      "40000  sentenses input, avarage loss: 0.41194034728221596\n",
      "50000  sentenses input, avarage loss: 0.422612299695611\n",
      "60000  sentenses input, avarage loss: 0.4304233816638589\n",
      "70000  sentenses input, avarage loss: 0.4315130202897957\n",
      "80000  sentenses input, avarage loss: 0.43232289573177696\n",
      "90000  sentenses input, avarage loss: 0.43233855687909656\n",
      "100000  sentenses input, avarage loss: 0.44013097519427535\n",
      "110000  sentenses input, avarage loss: 0.43974193327128885\n",
      "120000  sentenses input, avarage loss: 0.43623203369788827\n",
      "130000  sentenses input, avarage loss: 0.43391912738004557\n",
      "140000  sentenses input, avarage loss: 0.4306619136008833\n",
      "150000  sentenses input, avarage loss: 0.43411947452028593\n",
      "160000  sentenses input, avarage loss: 0.433748615719378\n",
      "170000  sentenses input, avarage loss: 0.43318295231636833\n",
      "180000  sentenses input, avarage loss: 0.43462322941670817\n",
      "190000  sentenses input, avarage loss: 0.4338897509735666\n",
      "200000  sentenses input, avarage loss: 0.43422089561633764\n",
      "[epochs 19 / 50 ], avarage loss: 0.43422089561633764\n",
      "10000  sentenses input, avarage loss: 0.3887871873378754\n",
      "20000  sentenses input, avarage loss: 0.4254199801757932\n",
      "30000  sentenses input, avarage loss: 0.4339820519338051\n",
      "40000  sentenses input, avarage loss: 0.4321103709470481\n",
      "50000  sentenses input, avarage loss: 0.4326299526914954\n",
      "60000  sentenses input, avarage loss: 0.4346801133702199\n",
      "70000  sentenses input, avarage loss: 0.43323712424508165\n",
      "80000  sentenses input, avarage loss: 0.42700648163445293\n",
      "90000  sentenses input, avarage loss: 0.42693247109651566\n",
      "100000  sentenses input, avarage loss: 0.42989974705129863\n",
      "110000  sentenses input, avarage loss: 0.43061658337373626\n",
      "120000  sentenses input, avarage loss: 0.43273296010059614\n",
      "130000  sentenses input, avarage loss: 0.43229081741032693\n",
      "140000  sentenses input, avarage loss: 0.43256843535495654\n",
      "150000  sentenses input, avarage loss: 0.4302171049937606\n",
      "160000  sentenses input, avarage loss: 0.4311614083801396\n",
      "170000  sentenses input, avarage loss: 0.4303196305902127\n",
      "180000  sentenses input, avarage loss: 0.43097609447832735\n",
      "190000  sentenses input, avarage loss: 0.42902465363964437\n",
      "200000  sentenses input, avarage loss: 0.42796835920307785\n",
      "[epochs 20 / 50 ], avarage loss: 0.42796835920307785\n",
      "10000  sentenses input, avarage loss: 0.41250253610312937\n",
      "20000  sentenses input, avarage loss: 0.40790959952399136\n",
      "30000  sentenses input, avarage loss: 0.4228033755719662\n",
      "40000  sentenses input, avarage loss: 0.42572227012366054\n",
      "50000  sentenses input, avarage loss: 0.4250525105446577\n",
      "60000  sentenses input, avarage loss: 0.4238416913896799\n",
      "70000  sentenses input, avarage loss: 0.41785849793414986\n",
      "80000  sentenses input, avarage loss: 0.41890349389286713\n",
      "90000  sentenses input, avarage loss: 0.42085593052829307\n",
      "100000  sentenses input, avarage loss: 0.4227282717395574\n",
      "110000  sentenses input, avarage loss: 0.4250232740034434\n",
      "120000  sentenses input, avarage loss: 0.4216065902588889\n",
      "130000  sentenses input, avarage loss: 0.41932424331943574\n",
      "140000  sentenses input, avarage loss: 0.4182773853559047\n",
      "150000  sentenses input, avarage loss: 0.41668631601706146\n",
      "160000  sentenses input, avarage loss: 0.4169969070015941\n",
      "170000  sentenses input, avarage loss: 0.4168336109323975\n",
      "180000  sentenses input, avarage loss: 0.42004382028244436\n",
      "190000  sentenses input, avarage loss: 0.42055501541711\n",
      "200000  sentenses input, avarage loss: 0.41993889973778276\n",
      "[epochs 21 / 50 ], avarage loss: 0.41993889973778276\n",
      "10000  sentenses input, avarage loss: 0.41198418363928796\n",
      "20000  sentenses input, avarage loss: 0.4264882562123239\n",
      "30000  sentenses input, avarage loss: 0.4294667090351383\n",
      "40000  sentenses input, avarage loss: 0.4373588975984603\n",
      "50000  sentenses input, avarage loss: 0.4387307856231928\n",
      "60000  sentenses input, avarage loss: 0.4403102223947644\n",
      "70000  sentenses input, avarage loss: 0.4400242125615478\n",
      "80000  sentenses input, avarage loss: 0.4324147535953671\n",
      "90000  sentenses input, avarage loss: 0.4292304018057055\n",
      "100000  sentenses input, avarage loss: 0.429882924772799\n",
      "110000  sentenses input, avarage loss: 0.42437012333761565\n",
      "120000  sentenses input, avarage loss: 0.4251250736260166\n",
      "130000  sentenses input, avarage loss: 0.4248609196881835\n",
      "140000  sentenses input, avarage loss: 0.4264889652681138\n",
      "150000  sentenses input, avarage loss: 0.42585246301442387\n",
      "160000  sentenses input, avarage loss: 0.4251917216624133\n",
      "170000  sentenses input, avarage loss: 0.42804984429522475\n",
      "180000  sentenses input, avarage loss: 0.42762997409742737\n",
      "190000  sentenses input, avarage loss: 0.43039321829809957\n",
      "200000  sentenses input, avarage loss: 0.4284436985310167\n",
      "[epochs 22 / 50 ], avarage loss: 0.4284436985310167\n",
      "10000  sentenses input, avarage loss: 0.4236062263324857\n",
      "20000  sentenses input, avarage loss: 0.4307433233130723\n",
      "30000  sentenses input, avarage loss: 0.4370226547929148\n",
      "40000  sentenses input, avarage loss: 0.42670945222023876\n",
      "50000  sentenses input, avarage loss: 0.4359881192483008\n",
      "60000  sentenses input, avarage loss: 0.43745174710638823\n",
      "70000  sentenses input, avarage loss: 0.4351526953998421\n",
      "80000  sentenses input, avarage loss: 0.42985664507141336\n",
      "90000  sentenses input, avarage loss: 0.4316665138035185\n",
      "100000  sentenses input, avarage loss: 0.4341661502476782\n",
      "110000  sentenses input, avarage loss: 0.4344069591689516\n",
      "120000  sentenses input, avarage loss: 0.43486493882878374\n",
      "130000  sentenses input, avarage loss: 0.43211902180686595\n",
      "140000  sentenses input, avarage loss: 0.4297668009796845\n",
      "150000  sentenses input, avarage loss: 0.43103372437382736\n",
      "160000  sentenses input, avarage loss: 0.43269399978104045\n",
      "170000  sentenses input, avarage loss: 0.43372410268358447\n",
      "180000  sentenses input, avarage loss: 0.43504424412941767\n",
      "190000  sentenses input, avarage loss: 0.43427923376917055\n",
      "200000  sentenses input, avarage loss: 0.4349401829401031\n",
      "[epochs 23 / 50 ], avarage loss: 0.4349401829401031\n",
      "10000  sentenses input, avarage loss: 0.43301066391170023\n",
      "20000  sentenses input, avarage loss: 0.43329521745443345\n",
      "30000  sentenses input, avarage loss: 0.42833406731486323\n",
      "40000  sentenses input, avarage loss: 0.4322335042525083\n",
      "50000  sentenses input, avarage loss: 0.4361503676697612\n",
      "60000  sentenses input, avarage loss: 0.4401787154376507\n",
      "70000  sentenses input, avarage loss: 0.43452194300613234\n",
      "80000  sentenses input, avarage loss: 0.43485413550864904\n",
      "90000  sentenses input, avarage loss: 0.436964964059492\n",
      "100000  sentenses input, avarage loss: 0.43190587892755866\n",
      "110000  sentenses input, avarage loss: 0.4308030940422958\n",
      "120000  sentenses input, avarage loss: 0.4310352141627421\n",
      "130000  sentenses input, avarage loss: 0.43140888092322993\n",
      "140000  sentenses input, avarage loss: 0.43203908027548876\n",
      "150000  sentenses input, avarage loss: 0.4333988085289796\n",
      "160000  sentenses input, avarage loss: 0.4339116294728592\n",
      "170000  sentenses input, avarage loss: 0.43343397202079786\n",
      "180000  sentenses input, avarage loss: 0.43330609661423497\n",
      "190000  sentenses input, avarage loss: 0.43390769216967257\n",
      "200000  sentenses input, avarage loss: 0.43191910287737845\n",
      "[epochs 24 / 50 ], avarage loss: 0.43191910287737845\n",
      "10000  sentenses input, avarage loss: 0.4113106827437878\n",
      "20000  sentenses input, avarage loss: 0.4268929570354521\n",
      "30000  sentenses input, avarage loss: 0.4301009098812938\n",
      "40000  sentenses input, avarage loss: 0.43445210105739535\n",
      "50000  sentenses input, avarage loss: 0.44110317081958056\n",
      "60000  sentenses input, avarage loss: 0.44117553867399695\n",
      "70000  sentenses input, avarage loss: 0.4408362215171967\n",
      "80000  sentenses input, avarage loss: 0.4420385621441528\n",
      "90000  sentenses input, avarage loss: 0.44227655973285435\n",
      "100000  sentenses input, avarage loss: 0.4435706427246332\n",
      "110000  sentenses input, avarage loss: 0.43498446077447045\n",
      "120000  sentenses input, avarage loss: 0.4379543120072534\n",
      "130000  sentenses input, avarage loss: 0.4372139677921167\n",
      "140000  sentenses input, avarage loss: 0.4391498368180224\n",
      "150000  sentenses input, avarage loss: 0.43834507920344673\n",
      "160000  sentenses input, avarage loss: 0.4366665139398538\n",
      "170000  sentenses input, avarage loss: 0.4357565136571579\n",
      "180000  sentenses input, avarage loss: 0.43502397110271784\n",
      "190000  sentenses input, avarage loss: 0.434488144797322\n",
      "200000  sentenses input, avarage loss: 0.4323861403353512\n",
      "[epochs 25 / 50 ], avarage loss: 0.4323861403353512\n",
      "10000  sentenses input, avarage loss: 0.4286525946855545\n",
      "20000  sentenses input, avarage loss: 0.4500750522315502\n",
      "30000  sentenses input, avarage loss: 0.4418206584453583\n",
      "40000  sentenses input, avarage loss: 0.43887141363695265\n",
      "50000  sentenses input, avarage loss: 0.43300111550837755\n",
      "60000  sentenses input, avarage loss: 0.4358747329004109\n",
      "70000  sentenses input, avarage loss: 0.4346410956819143\n",
      "80000  sentenses input, avarage loss: 0.43332728048786523\n",
      "90000  sentenses input, avarage loss: 0.43247840240597724\n",
      "100000  sentenses input, avarage loss: 0.43361060581728816\n",
      "110000  sentenses input, avarage loss: 0.42587407244538716\n",
      "120000  sentenses input, avarage loss: 0.4307967684449007\n",
      "130000  sentenses input, avarage loss: 0.4303649366159852\n",
      "140000  sentenses input, avarage loss: 0.4308628405790244\n",
      "150000  sentenses input, avarage loss: 0.43025366098682083\n",
      "160000  sentenses input, avarage loss: 0.43020962077658625\n",
      "170000  sentenses input, avarage loss: 0.4289525214845643\n",
      "180000  sentenses input, avarage loss: 0.4313599360899793\n",
      "190000  sentenses input, avarage loss: 0.43127681344355406\n",
      "200000  sentenses input, avarage loss: 0.43092295992560686\n",
      "[epochs 26 / 50 ], avarage loss: 0.43092295992560686\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2f6f5fba81f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"begin to train model with labeled sentenses...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = sentiment.train_model(\n\u001b[0;32m----> 4\u001b[0;31m     sentenses_labeled, labels_train, wordvecs.wv, DIM_WORD, epochs = 50)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"computing the acurracy of training set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml_hws/hw4/sentiment.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(sentenses, labels, wv, dim_word, model, epochs, nbatch)\u001b[0m\n\u001b[1;32m     85\u001b[0m                       \"], avarage loss:\", loss_all[i])\n\u001b[1;32m     86\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 用有标签的训练集进行一次训练\n",
    "print(\"begin to train model with labeled sentenses...\")\n",
    "model = sentiment.train_model(\n",
    "    sentenses_labeled, labels_train, wordvecs.wv, DIM_WORD, epochs = 50)\n",
    "\n",
    "print(\"computing the acurracy of training set...\")\n",
    "y_pred = sentiment.predict(model, sentenses_labeled, wordvecs.wv).squeeze()\n",
    "labels_pred = utils.p2label(y_pred)\n",
    "print(\"the accuracy of labeled training set is \", 100 *\n",
    "      utils.calc_accuracy(labels_train, labels_pred), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对没有标签的训练集进行一次预测，当预测值大于阈值thresh的时候，将其加入到再训练集合\n",
    "thresh = 0.9\n",
    "print(\"generating labels from unlabeled train data with threshold \", thresh, \" ...\")\n",
    "y_pred_unlabeled = sentiment.predict(\n",
    "    model, sentenses_nolabeled, wordvecs.wv).squeeze()\n",
    "# 只取结果概率在区间[0, 1-thresh]和[thresh,1]中的预测为可信的预测\n",
    "idx = ((y_pred_unlabeled > thresh) | (y_pred_unlabeled < (1-thresh))).numpy()\n",
    "lables_generated = np.array(utils.p2label(y_pred_unlabeled[idx]))\n",
    "sentenses_generated = sentenses_nolabeled[idx]\n",
    "# 用这些通过半监督得到的label加入到原来的有labeled数据集中，继续训练模型\n",
    "print(len(lables_generated), \" labels have been generated.\")\n",
    "print(\"begin to train model with generated labels...\")\n",
    "model = sentiment.train_model(\n",
    "    np.concatenate((sentenses_labeled, sentenses_generated),axis=0), \n",
    "    np.concatenate((labels_train,lables_generated), axis=0),\n",
    "    wordvecs.wv, DIM_WORD, epochs=10,\n",
    "    model = model)#继续之前的模型训练\n",
    "print(\"computing the acurracy of training set...\")\n",
    "y_pred = sentiment.predict(model, sentenses_labeled, wordvecs.wv).squeeze()\n",
    "labels_pred = utils.p2label(y_pred)\n",
    "print(\"the accuracy of labeled training set with modified model is \", 100 *\n",
    "      utils.calc_accuracy(labels_train, labels_pred), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试写入结果文档\n",
    "print(\"waiting for testing ...\")\n",
    "y_pred_test = sentiment.predict(\n",
    "    model, sentenses_test, wordvecs.wv).squeeze().numpy()\n",
    "label_pred_test = utils.p2label(y_pred_test)\n",
    "pd.DataFrame({\"id\": [x for x in range(len(label_pred_test))],\n",
    "              \"label\":  label_pred_test}).to_csv(\"./data/result.csv\", index=False)\n",
    "print(\"test result has been written into ./data/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}